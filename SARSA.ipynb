{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# we have an agent \n",
    "# the agent will be able to move through a matrix\n",
    "#the matrix is a 10x15 space matrix\n",
    "# there are locks thet the agent cannot move on to, these blocks are marked with an x\n",
    "#agents can move up down left and right\n",
    "# we need a history table called history with 3 columns, x position, y position and the reweard/punishment\n",
    "\n",
    "agent= {\"x\":1,\"y\":-1}\n",
    "table=[]\n",
    "table.append(agent.copy())\n",
    "action= [\"Moved down\"]\n",
    "learning_rate = 0.1\n",
    "gamma= 0.95\n",
    "e_greedy=0.1\n",
    "\n",
    "\n",
    "rewards=np.ones([10,15])\n",
    "rewards=rewards*-0.01\n",
    "rewards[9][12]= 1\n",
    "\n",
    "actions=[\"Moved up\",\"Moved down\",\"Moved left\",\"Moved right\"]\n",
    "\n",
    "q_table=[[0,0,\"none\",0]]\n",
    "\n",
    "#actions for each state\n",
    "for actionz in actions:\n",
    "    # y values\n",
    "    for i in range(1,11):\n",
    "        # x values\n",
    "        for j in range(1,16):\n",
    "            #append all three witha  value of 0 for initialization\n",
    "            q_table.append(list([j,i,actionz,0]))\n",
    "q_table.pop(0)\n",
    "q_table= pd.DataFrame.from_records(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#functions to move\n",
    "# current position of an agent is (x_t,y_t)\n",
    "def move_up(agent):\n",
    "    agent[\"y\"]+=1   \n",
    "    print(\"Moved up\")\n",
    "    action.append(\"Moved up\")\n",
    "    return(agent)\n",
    "def move_down(agent):\n",
    "    agent[\"y\"]-=1  \n",
    "    print(\"Moved down\")\n",
    "    action.append(\"Moved down\")\n",
    "    return(agent)\n",
    "def move_right(agent):\n",
    "    agent[\"x\"]+=1   \n",
    "    print(\"Moved right\")\n",
    "    action.append(\"Moved right\")\n",
    "    return(agent)\n",
    "def move_left(agent):\n",
    "    agent[\"x\"]-=1   \n",
    "    print(\"Moved left\")\n",
    "    action.append(\"Moved left\")\n",
    "    return(agent)\n",
    "\n",
    "# function for hitting a wall\n",
    "def hit_wall(agent,table):\n",
    "    agent= table[-1]\n",
    "    print(\"hit wall\")\n",
    "    return(agent)\n",
    "\n",
    "# create a table of rewards for each point in the maze\n",
    "end_point= [13,-10]\n",
    "wall_points=[[3,-2],[3,-3],[3,-4],[1,-6],[2,-6],[3,-6],[4,-6],[5,-6],[6,-6],[7,-6],[8,-6]\n",
    "             ,[14,-2],[14,-3],[14,-4],[14,-5],[14,-6],[14,-7],[10,-9],[11,-9],[12,-9],[13,-9],[14,-9],[15,-9],[7,-7],[7,-8]]\n",
    "\n",
    "# create a function to implemant the action choices\n",
    "def move_bi(agent,q_table, rand,rand2,end_point=end_point,wall_points=wall_points,table=table, greedy = 0.1,gamma=0.95):\n",
    "\n",
    "    if rand2 >0 and rand2<=greedy:\n",
    "        if rand >=0 and rand<=1 :\n",
    "            agent=move_up(agent)\n",
    "        if rand >=0.25 and rand<0.5 :\n",
    "            agent=move_down(agent)\n",
    "        if rand >=0.5 and rand<0.75 :\n",
    "            agent=move_left(agent)\n",
    "        if rand >=0.75 and rand<1 :\n",
    "            agent= move_right(agent)\n",
    "    else: \n",
    "    #greedy algo\n",
    "        indices = [i for i, x in enumerate(q_table[(q_table[0]==agent[\"x\"]) & (q_table[1]==-agent[\"y\"])][3]) if x == max(q_table[(q_table[0]==agent[\"x\"]) & (q_table[1]==-agent[\"y\"])][3])]\n",
    "        chosen_action=list(q_table[(q_table[0]==agent[\"x\"]) & (q_table[1]==-agent[\"y\"])][2])[random.choice(indices)]\n",
    "\n",
    "        if chosen_action == \"Moved up\":\n",
    "            agent=move_up(agent)\n",
    "        elif chosen_action == \"Moved down\":\n",
    "            agent=move_down(agent)\n",
    "        elif chosen_action == \"Moved left\":\n",
    "            agent=move_left(agent)\n",
    "        elif chosen_action == \"Moved right\":\n",
    "            agent=move_right(agent)\n",
    "\n",
    "    return(agent)\n",
    "\n",
    "\n",
    "def move_bi_action(agent,q_table, rand,rand2,end_point=end_point,wall_points=wall_points,table=table, greedy = 0.1,gamma=0.95):\n",
    "    #print(rand)\n",
    "    #print(rand2)\n",
    "    if rand2 >0 and rand2<=greedy:\n",
    "        #rand=random.random()\n",
    "        if rand >=0 and rand<=1 :\n",
    "            chosen_action = \"Moved up\"\n",
    "        if rand >=0.25 and rand<0.5 :\n",
    "            chosen_action = \"Moved down\"\n",
    "        if rand >=0.5 and rand<0.75 :\n",
    "            chosen_action = \"Moved left\"\n",
    "        if rand >=0.75 and rand<1 :\n",
    "            chosen_action = \"Moved right\"\n",
    "    else: \n",
    "    #greedy algo\n",
    "        indices = [i for i, x in enumerate(q_table[(q_table[0]==agent[\"x\"]) & (q_table[1]==-agent[\"y\"])][3]) if x == max(q_table[(q_table[0]==agent[\"x\"]) & (q_table[1]==-agent[\"y\"])][3])]\n",
    "        chosen_action=list(q_table[(q_table[0]==agent[\"x\"]) & (q_table[1]==-agent[\"y\"])][2])[random.choice(indices)]\n",
    "\n",
    "    return(chosen_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "epoch=0\n",
    "n_iter=1000\n",
    "steps=[0]\n",
    "solved=0\n",
    "while epoch<n_iter:\n",
    "    solved=0\n",
    "    step=0\n",
    "    agent= {\"x\":1,\"y\":-1}\n",
    "    table=[]\n",
    "    table.append(agent.copy())\n",
    "    action= [\"Moved down\"]\n",
    "    rand1=random.random()\n",
    "    rand2=random.random()\n",
    "    print(agent)\n",
    "    chosen_action=move_bi_action(agent,q_table, rand1, rand2,end_point,wall_points,table,greedy=e_greedy) \n",
    "    table.append(agent.copy())\n",
    "    #call the function to return the first value for move here - init move value\n",
    "    while solved==0:\n",
    "        rand1=random.random()\n",
    "        rand2=random.random()\n",
    "        #print(q_table[(q_table[1]==-agent[\"y\"] ) & (q_table[0]==agent[\"x\"] )])\n",
    "        if chosen_action == \"Moved up\":\n",
    "            agent=move_up(agent)\n",
    "        elif chosen_action == \"Moved down\":\n",
    "            agent=move_down(agent)\n",
    "        elif chosen_action == \"Moved left\":\n",
    "            agent=move_left(agent)\n",
    "        elif chosen_action == \"Moved right\":\n",
    "            agent=move_right(agent)\n",
    "        \n",
    "        \n",
    "        if [agent[\"x\"],agent[\"y\"]] in wall_points:\n",
    "            agent=hit_wall(agent,table)\n",
    "\n",
    "        if agent[\"x\"]>15:\n",
    "            agent=hit_wall(agent,table)\n",
    "\n",
    "        if agent[\"x\"]<=0:\n",
    "            agent=hit_wall(agent,table)\n",
    "\n",
    "        if agent[\"y\"]<-10:\n",
    "            agent=hit_wall(agent,table)\n",
    "    \n",
    "        if agent[\"y\"]>=0:\n",
    "            agent=hit_wall(agent,table)\n",
    "        #make the current value for move\n",
    "\n",
    "        if [agent[\"x\"],agent[\"y\"]] == end_point:\n",
    "            #agent[\"reward\"]=1\n",
    "            print(\"solved\")\n",
    "            solved=1\n",
    "            #break\n",
    "        \n",
    "        #update the next move\n",
    "        chosen_action=move_bi_action(agent,q_table, rand1, rand2,end_point,wall_points,table,greedy=e_greedy) \n",
    "\n",
    "        table.append(agent.copy())\n",
    "        \n",
    "        Q_s_a=q_table[(q_table[1]==-table[-2][\"y\"]) & (q_table[0]==table[-2][\"x\"]) &(q_table[2]==action[-1])][3]\n",
    "        #Q_s_a=[element[3] for element in q_table if element[2]==action[-1] and element[0]==table[-1][\"x\"] \n",
    "        #and element[1]== -table[-1][\"y\"]]\n",
    "        #this needs to happen after the agent has moved and while updating the Q_value\n",
    "        max_q_sa_next=q_table[(q_table[1]==-agent[\"y\"] ) & (q_table[0]==agent[\"x\"] )&(q_table[2]==chosen_action)][3]\n",
    "\n",
    "        # Updating Q-values\n",
    "        q_table[3][(q_table[1]==-table[-2][\"y\"]) & (q_table[0]==table[-2][\"x\"]) &(q_table[2]==action[-1])]=\\\n",
    "        Q_s_a + learning_rate*(rewards[-table[-2][\"y\"]-1][table[-2][\"x\"]-1] +(gamma* max_q_sa_next.values[0]) - Q_s_a)\n",
    "        \n",
    "        step+=1\n",
    "#         print(epoch)\n",
    "#         print(agent)\n",
    "\n",
    "    epoch+=1\n",
    "    \n",
    "    steps.append(step)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create data\n",
    "x = [i for i in range(1,100)]\n",
    "y = steps[1:100]\n",
    "colors = (0,0,0)\n",
    "#area = np.pi*3\n",
    "\n",
    "# Plot\n",
    "plt.plot(x, y, c=colors, alpha=1)\n",
    "plt.title('Number of steps per Episode')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Number of steps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.colors as mlc\n",
    "import matplotlib.pyplot as mlp\n",
    "\n",
    "states = table_df[\"y\"]\n",
    "walk = table_df[\"x\"]\n",
    "\n",
    "states_len = len(states)\n",
    "walk_len = len(walk)\n",
    "\n",
    "img = np.zeros((states_len, walk_len), dtype=float)\n",
    "\n",
    "for i, s in enumerate(walk):\n",
    "    img[s, i] = 1.0\n",
    "\n",
    "figure, ax = mlp.subplots()\n",
    "\n",
    "color_map = mlc.LinearSegmentedColormap.from_list('ColorMap', [(1.000, 1.000, 1.000), (0.984, 0.501, 0.447)])\n",
    "ax.imshow(img, cmap=color_map, interpolation='none')\n",
    "\n",
    "ax.set_xlabel('Steps', fontsize=13)\n",
    "ax.set_xticks(np.arange(0, walk_len, 1))\n",
    "ax.set_xticks(np.arange(-0.5, walk_len, 1), minor=True)\n",
    "ax.set_xticklabels(np.arange(1, walk_len + 1, 1))\n",
    "\n",
    "ax.set_ylabel('States', fontsize=13)\n",
    "ax.set_yticks(np.arange(0, states_len, 1))\n",
    "ax.set_yticks(np.arange(-.5, states_len, 1), minor=True)\n",
    "ax.set_yticklabels(states)\n",
    "\n",
    "ax.grid(which='minor', color='k')\n",
    "\n",
    "ax.set_title('Walkplot (Sequence)', fontsize=15, fontweight='bold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final run after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just run for a single iteration after training\n",
    "%matplotlib inline\n",
    "epoch=0\n",
    "n_iter=1\n",
    "#steps=[0]\n",
    "solved=0\n",
    "while epoch<n_iter:\n",
    "    solved=0\n",
    "    #step=0\n",
    "    agent= {\"x\":1,\"y\":-1}\n",
    "    table=[]\n",
    "    table.append(agent.copy())\n",
    "    action= [\"Moved down\"]\n",
    "    while solved==0:\n",
    "        rand1=random.random()\n",
    "        rand2=random.random()\n",
    "        agent=move_bi(agent,q_table, rand1, rand2,end_point,wall_points,table,greedy=0)\n",
    "\n",
    "        if [agent[\"x\"],agent[\"y\"]] in wall_points:\n",
    "            agent=hit_wall(agent,table)\n",
    "\n",
    "        if agent[\"x\"]>15:\n",
    "            agent=hit_wall(agent,table)\n",
    "\n",
    "        if agent[\"x\"]<=0:\n",
    "            agent=hit_wall(agent,table)\n",
    "\n",
    "        if agent[\"y\"]<-10:\n",
    "            agent=hit_wall(agent,table)\n",
    "\n",
    "        if agent[\"y\"]>=0:\n",
    "            agent=hit_wall(agent,table)\n",
    "\n",
    "        if [agent[\"x\"],agent[\"y\"]] == end_point:\n",
    "            #agent[\"reward\"]=1\n",
    "            print(\"solved\")\n",
    "            solved=1\n",
    "            #break\n",
    "\n",
    "        #else: agent[\"reward\"]=-0.01\n",
    "\n",
    "        table.append(agent.copy())\n",
    "        Q_s_a=q_table[(q_table[1]==-table[-2][\"y\"]) & (q_table[0]==table[-2][\"x\"]) &(q_table[2]==action[-1])][3]\n",
    "\n",
    "        #this needs to happen after the agent has moved and while updating the Q_value\n",
    "        max_q_sa_next=q_table[(q_table[1]==-agent[\"y\"] ) & (q_table[0]==agent[\"x\"] )][3].max()\n",
    "\n",
    "        #print(q_table[(q_table[0]==1)&(q_table[1]==1)])\n",
    "        # Updating Q-values\n",
    "        q_table[3][(q_table[1]==-table[-2][\"y\"]) & (q_table[0]==table[-2][\"x\"]) &(q_table[2]==action[-1])]=\\\n",
    "        Q_s_a + learning_rate*(rewards[-table[-1][\"y\"]-1][table[-1][\"x\"]-1] +(gamma* max_q_sa_next) - Q_s_a)\n",
    "        \n",
    "        step+=1\n",
    "#         print(epoch)\n",
    "#         print(agent)\n",
    "\n",
    "    epoch+=1\n",
    "    \n",
    "    #steps.append(step)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.pyplot.grid(b=True, which='major', axis='both')\n",
    "matplotlib.pyplot.grid(b=True, which='minor', axis='both')\n",
    "plt.scatter(pd.DataFrame(table)[\"x\"],pd.DataFrame(table)[\"y\"])\n",
    "plt.plot(pd.DataFrame(table)[\"x\"],pd.DataFrame(table)[\"y\"])\n",
    "plt.scatter(pd.DataFrame(wall_points)[0],pd.DataFrame(wall_points)[1],color='red')\n",
    "plt.yticks([i for i in range(-11,1)])\n",
    "plt.xticks([i for i in range(0,16)])\n",
    "\n",
    "plt.title('Agent Path')\n",
    "plt.xlabel('X-axis')\n",
    "plt.ylabel('y-axis')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
